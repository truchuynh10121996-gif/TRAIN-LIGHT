"""
===========================================================================
üè¶ SYNTHETIC DATA GENERATOR CHO LIGHTGBM CH·ªêNG GIAN L·∫¨N & L·ª™A ƒê·∫¢O - VI·ªÜT NAM
===========================================================================
·ª®ng d·ª•ng Streamlit t·∫°o d·ªØ li·ªáu gi·∫£ l·∫≠p chu·∫©n h√†nh vi ng∆∞·ªùi Vi·ªát Nam
ƒë·ªÉ train m√¥ h√¨nh LightGBM ph√°t hi·ªán:
- GIAN L·∫¨N (Fraud): Account Takeover, Mule Account, Card Testing
- L·ª™A ƒê·∫¢O (Scam): Romance Scam, Investment Scam, Impersonation (gi·∫£ c√¥ng an/ng√¢n h√†ng)

Author: Data Engineering Team - Vietnam Banking
Version: 2.0.0 - Optimized for 500K+ transactions
===========================================================================
"""

import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import random
from collections import defaultdict
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

# ===========================================================================
# C·∫§U H√åNH M·∫∂C ƒê·ªäNH
# ===========================================================================
DEFAULT_N_TRANSACTIONS = 100_000
DEFAULT_FRAUD_RATE = 0.05
DEFAULT_N_USERS = 5000
DEFAULT_N_RECIPIENTS = 8000
RANDOM_SEED = 42

# ===========================================================================
# CONSTANTS - H√ÄNH VI NG∆Ø·ªúI VI·ªÜT NAM
# ===========================================================================

# C√°c m·ª©c ti·ªÅn ph·ªï bi·∫øn t·∫°i Vi·ªát Nam (ƒë∆°n v·ªã: VND)
COMMON_AMOUNTS_VN = [
    50_000, 100_000, 150_000, 200_000, 300_000, 500_000,
    1_000_000, 2_000_000, 3_000_000, 5_000_000,
    10_000_000, 20_000_000, 50_000_000
]

# C√°c lo·∫°i giao d·ªãch ph·ªï bi·∫øn t·∫°i VN
TRANSACTION_TYPES = {
    'chuyen_noi_bo': 0.30,        # Chuy·ªÉn kho·∫£n n·ªôi b·ªô ng√¢n h√†ng
    'chuyen_lien_ngan_hang': 0.25, # Chuy·ªÉn li√™n ng√¢n h√†ng (Napas)
    'thanh_toan_hoa_don': 0.15,    # ƒêi·ªán/n∆∞·ªõc/internet
    'topup_vi': 0.12,              # N·∫°p v√≠ Momo/ZaloPay/VNPay
    'rut_atm': 0.08,               # R√∫t ATM
    'thanh_toan_pos': 0.05,        # Qu·∫πt th·∫ª POS
    'hoc_phi_vien_phi': 0.03,      # H·ªçc ph√≠, vi·ªán ph√≠
    'mua_hang_online': 0.02        # Mua h√†ng online
}

# K√™nh giao d·ªãch
CHANNELS = {
    'mobile_app': 0.55,  # ƒêa s·ªë ng∆∞·ªùi VN d√πng app mobile
    'web': 0.20,
    'atm': 0.15,
    'pos': 0.10
}

# Channel risk theo quy t·∫Øc ng√¢n h√†ng VN
CHANNEL_RISK_BASE = {
    'mobile_app': 0.35,
    'web': 0.25,
    'atm': 0.10,
    'pos': 0.05
}

# Transaction type risk
TX_TYPE_RISK_BASE = {
    'chuyen_lien_ngan_hang': 0.40,
    'chuyen_noi_bo': 0.20,
    'topup_vi': 0.15,
    'mua_hang_online': 0.12,
    'thanh_toan_hoa_don': 0.05,
    'rut_atm': 0.08,
    'thanh_toan_pos': 0.06,
    'hoc_phi_vien_phi': 0.04
}

# 3 v√πng ƒë·ªãa l√Ω Vi·ªát Nam
GEO_REGIONS = {
    'bac': {'center': (21.0285, 105.8542), 'weight': 0.35},  # H√† N·ªôi
    'trung': {'center': (16.0544, 108.2022), 'weight': 0.15},  # ƒê√† N·∫µng
    'nam': {'center': (10.8231, 106.6297), 'weight': 0.50}   # HCM
}


# ===========================================================================
# SECTION 1: GENERATE BASE TRANSACTIONS
# ===========================================================================

def generate_user_profiles(n_users, seed=RANDOM_SEED):
    """
    T·∫°o profile ng∆∞·ªùi d√πng v·ªõi ph√¢n ph·ªëi Pareto (10% user chi·∫øm 60% giao d·ªãch)
    """
    np.random.seed(seed)

    users = []
    for i in range(n_users):
        user_id = f"USR_{i:06d}"

        # Ph√¢n b·ªë v√πng mi·ªÅn
        region = np.random.choice(
            list(GEO_REGIONS.keys()),
            p=[GEO_REGIONS[r]['weight'] for r in GEO_REGIONS.keys()]
        )

        # Thi·∫øt b·ªã ch√≠nh c·ªßa user (ng∆∞·ªùi VN √≠t ƒë·ªïi thi·∫øt b·ªã)
        primary_device = f"DEV_{np.random.randint(100000, 999999)}"

        # S·ªë ng√†y ƒë√£ m·ªü t√†i kho·∫£n (30 - 3650 ng√†y)
        account_age = np.random.exponential(scale=500) + 30
        account_age = min(account_age, 3650)

        # M·ª©c thu nh·∫≠p ∆∞·ªõc t√≠nh (·∫£nh h∆∞·ªüng ƒë·∫øn amount trung b√¨nh)
        # Ph√¢n ph·ªëi log-normal cho thu nh·∫≠p VN
        income_level = np.random.lognormal(mean=2.5, sigma=0.8)
        income_level = np.clip(income_level, 0.5, 20)

        # Gi·ªù giao d·ªãch quen thu·ªôc c·ªßa user (mean hour)
        preferred_hour = np.random.choice([9, 10, 14, 15, 20, 21], p=[0.15, 0.20, 0.15, 0.15, 0.20, 0.15])
        preferred_hour += np.random.uniform(-1, 1)

        # Activity level (Pareto: 10% power users)
        if np.random.random() < 0.10:
            activity_weight = np.random.uniform(5, 15)  # Power users
        elif np.random.random() < 0.30:
            activity_weight = np.random.uniform(1, 5)   # Regular users
        else:
            activity_weight = np.random.uniform(0.1, 1) # Low activity users

        users.append({
            'user_id': user_id,
            'region': region,
            'primary_device': primary_device,
            'account_age_days': int(account_age),
            'income_level': income_level,
            'preferred_hour': preferred_hour,
            'activity_weight': activity_weight
        })

    return pd.DataFrame(users)


def generate_recipient_profiles(n_recipients, seed=RANDOM_SEED):
    """
    T·∫°o profile ng∆∞·ªùi nh·∫≠n ti·ªÅn
    95% ng∆∞·ªùi nh·∫≠n ch·ªâ nh·∫≠n t·ª´ 1-3 ng∆∞·ªùi quen
    """
    np.random.seed(seed)

    recipients = []
    for i in range(n_recipients):
        recipient_id = f"RCP_{i:06d}"

        # S·ªë ng∆∞·ªùi g·ª≠i t·ªëi ƒëa cho recipient n√†y (95% ch·ªâ 1-3 ng∆∞·ªùi)
        if np.random.random() < 0.95:
            max_senders = np.random.randint(1, 4)
        else:
            max_senders = np.random.randint(4, 50)  # C√≥ th·ªÉ l√† mule

        # ƒê√°nh d·∫•u ti·ªÅm nƒÉng mule (nh·∫≠n t·ª´ nhi·ªÅu ng∆∞·ªùi)
        is_potential_mule = max_senders > 20

        recipients.append({
            'recipient_id': recipient_id,
            'max_senders': max_senders,
            'is_potential_mule': is_potential_mule
        })

    return pd.DataFrame(recipients)


def generate_amount_vn(income_level, tx_type, is_salary_period=False):
    """
    Sinh s·ªë ti·ªÅn giao d·ªãch theo h√†nh vi ng∆∞·ªùi Vi·ªát Nam
    - ƒêa s·ªë l√† s·ªë ch·∫µn ngh√¨n
    - Ph√¢n ph·ªëi theo lo·∫°i giao d·ªãch v√† thu nh·∫≠p
    """
    # Base amount theo lo·∫°i giao d·ªãch
    if tx_type == 'thanh_toan_hoa_don':
        # H√≥a ƒë∆°n ƒëi·ªán/n∆∞·ªõc/internet: 100k - 2 tri·ªáu
        base = np.random.choice([100_000, 200_000, 300_000, 500_000, 800_000, 1_000_000, 1_500_000])
    elif tx_type == 'topup_vi':
        # N·∫°p v√≠: 50k - 2 tri·ªáu
        base = np.random.choice([50_000, 100_000, 200_000, 500_000, 1_000_000, 2_000_000])
    elif tx_type == 'rut_atm':
        # R√∫t ATM: th∆∞·ªùng ch·∫µn 500k, 1tr, 2tr
        base = np.random.choice([500_000, 1_000_000, 2_000_000, 3_000_000, 5_000_000])
    elif tx_type == 'hoc_phi_vien_phi':
        # H·ªçc ph√≠/vi·ªán ph√≠: l·ªõn h∆°n
        base = np.random.choice([500_000, 1_000_000, 2_000_000, 5_000_000, 10_000_000, 20_000_000])
    elif tx_type in ['chuyen_noi_bo', 'chuyen_lien_ngan_hang']:
        # Chuy·ªÉn kho·∫£n: ƒëa d·∫°ng
        base = np.random.choice(COMMON_AMOUNTS_VN, p=[
            0.08, 0.12, 0.08, 0.15, 0.10, 0.17,  # 50k-500k (t·ªïng: 0.70)
            0.12, 0.08, 0.04, 0.03,               # 1tr-5tr (t·ªïng: 0.27)
            0.02, 0.007, 0.003                    # 10tr-50tr (t·ªïng: 0.03)
        ])
    else:
        # Mua h√†ng online, POS
        base = np.random.choice([50_000, 100_000, 200_000, 300_000, 500_000, 1_000_000])

    # ƒêi·ªÅu ch·ªânh theo income level
    amount = base * (0.5 + income_level * 0.3)

    # TƒÉng amount trong k·ª≥ l∆∞∆°ng
    if is_salary_period and np.random.random() < 0.3:
        amount *= np.random.uniform(1.5, 3.0)

    # L√†m tr√≤n ngh√¨n ƒë·ªìng (y√™u c·∫ßu b·∫Øt bu·ªôc)
    amount = int(round(amount / 1000) * 1000)

    # Gi·ªõi h·∫°n h·ª£p l√Ω
    amount = max(10_000, min(amount, 500_000_000))

    return amount


def generate_transaction_hour_vn(is_fraud=False, preferred_hour=None):
    """
    Sinh gi·ªù giao d·ªãch theo h√†nh vi ng∆∞·ªùi Vi·ªát Nam
    - Cao ƒëi·ªÉm: 8-11h, 14-16h, 19:30-21:30
    - G·∫ßn nh∆∞ t·∫Øt sau 23h (tr·ª´ fraud)
    """
    if is_fraud and np.random.random() < 0.4:
        # Fraud th∆∞·ªùng x·∫£y ra ban ƒë√™m 1-4 AM
        return np.random.randint(1, 5) + np.random.random()

    # Ph√¢n ph·ªëi gi·ªù b√¨nh th∆∞·ªùng c·ªßa ng∆∞·ªùi Vi·ªát
    hour_weights = np.array([
        0.005, 0.002, 0.001, 0.001, 0.002, 0.005,  # 0-5h: r·∫•t √≠t
        0.02, 0.04, 0.08, 0.10, 0.10, 0.08,        # 6-11h: tƒÉng d·∫ßn, cao ƒëi·ªÉm s√°ng
        0.05, 0.04, 0.08, 0.09, 0.07, 0.05,        # 12-17h: cao ƒëi·ªÉm chi·ªÅu
        0.04, 0.06, 0.09, 0.08, 0.04, 0.01         # 18-23h: cao ƒëi·ªÉm t·ªëi, gi·∫£m d·∫ßn
    ])
    hour_weights = hour_weights / hour_weights.sum()

    hour = np.random.choice(24, p=hour_weights)
    minute = np.random.randint(0, 60)

    # ƒêi·ªÅu ch·ªânh theo preferred hour c·ªßa user
    if preferred_hour is not None and np.random.random() < 0.3:
        hour = int(preferred_hour) % 24

    return hour + minute / 60


def is_salary_period(date):
    """
    Ki·ªÉm tra c√≥ ph·∫£i k·ª≥ l∆∞∆°ng kh√¥ng (ng√†y 25 - ng√†y 5 th√°ng sau)
    """
    day = date.day
    return day >= 25 or day <= 5


def is_bill_period(date):
    """
    Ki·ªÉm tra c√≥ ph·∫£i k·ª≥ thanh to√°n h√≥a ƒë∆°n kh√¥ng (ƒë·∫ßu th√°ng 1-10)
    """
    return date.day <= 10


def generate_base_transactions(n_transactions, n_users, n_recipients, seed=RANDOM_SEED):
    """
    T·∫°o d·ªØ li·ªáu giao d·ªãch c∆° b·∫£n theo h√†nh vi ng∆∞·ªùi Vi·ªát Nam
    """
    np.random.seed(seed)
    random.seed(seed)

    # T·∫°o profiles
    user_profiles = generate_user_profiles(n_users, seed)
    recipient_profiles = generate_recipient_profiles(n_recipients, seed)

    # Sampling users theo activity weight (Pareto distribution)
    user_weights = user_profiles['activity_weight'].values
    user_weights = user_weights / user_weights.sum()

    transactions = []

    # T·∫°o mapping user -> recipients quen thu·ªôc (95% ch·ªâ chuy·ªÉn cho 1-3 ng∆∞·ªùi)
    user_familiar_recipients = {}
    for user_id in user_profiles['user_id']:
        n_familiar = np.random.choice([1, 2, 3, 4, 5], p=[0.3, 0.35, 0.25, 0.07, 0.03])
        familiar_rcps = np.random.choice(recipient_profiles['recipient_id'].values, size=n_familiar, replace=False)
        user_familiar_recipients[user_id] = list(familiar_rcps)

    # Base timestamp (1 nƒÉm g·∫ßn ƒë√¢y)
    end_date = datetime.now()
    start_date = end_date - timedelta(days=365)

    for i in range(n_transactions):
        # Ch·ªçn user theo weight
        user_idx = np.random.choice(len(user_profiles), p=user_weights)
        user = user_profiles.iloc[user_idx]
        user_id = user['user_id']

        # Sinh timestamp
        random_days = np.random.uniform(0, 365)
        tx_date = start_date + timedelta(days=random_days)

        # Ki·ªÉm tra k·ª≥ l∆∞∆°ng v√† k·ª≥ h√≥a ƒë∆°n
        is_salary = is_salary_period(tx_date)
        is_bill = is_bill_period(tx_date)

        # Sinh gi·ªù giao d·ªãch
        hour_decimal = generate_transaction_hour_vn(is_fraud=False, preferred_hour=user['preferred_hour'])
        hour = int(hour_decimal)
        minute = int((hour_decimal - hour) * 60)
        tx_datetime = tx_date.replace(hour=hour, minute=minute, second=np.random.randint(0, 60))

        # Ch·ªçn lo·∫°i giao d·ªãch
        tx_types = list(TRANSACTION_TYPES.keys())
        tx_probs = list(TRANSACTION_TYPES.values())

        # TƒÉng thanh to√°n h√≥a ƒë∆°n trong k·ª≥ bill
        if is_bill:
            tx_probs[tx_types.index('thanh_toan_hoa_don')] *= 2
            tx_probs = [p / sum(tx_probs) for p in tx_probs]

        tx_type = np.random.choice(tx_types, p=tx_probs)

        # Ch·ªçn k√™nh giao d·ªãch
        channel = np.random.choice(list(CHANNELS.keys()), p=list(CHANNELS.values()))

        # ATM ch·ªâ cho r√∫t ti·ªÅn
        if tx_type == 'rut_atm':
            channel = 'atm'
        elif channel == 'atm' and tx_type != 'rut_atm':
            channel = 'mobile_app'

        # Sinh s·ªë ti·ªÅn
        amount = generate_amount_vn(user['income_level'], tx_type, is_salary)

        # Ch·ªçn recipient
        # 90% chuy·ªÉn cho ng∆∞·ªùi quen, 10% ng∆∞·ªùi m·ªõi
        if np.random.random() < 0.90 and user_familiar_recipients[user_id]:
            recipient_id = np.random.choice(user_familiar_recipients[user_id])
            is_new_recipient = 0
        else:
            recipient_id = np.random.choice(recipient_profiles['recipient_id'].values)
            is_new_recipient = 1

        # Thi·∫øt b·ªã: 95% d√πng thi·∫øt b·ªã ch√≠nh
        if np.random.random() < 0.95:
            device_id = user['primary_device']
            is_new_device = 0
        else:
            device_id = f"DEV_{np.random.randint(100000, 999999)}"
            is_new_device = 1

        # V·ªã tr√≠ giao d·ªãch (t√≠nh location_diff_km)
        user_region = user['region']
        user_center = GEO_REGIONS[user_region]['center']

        # 85% giao d·ªãch t·∫°i v√πng c·ªßa m√¨nh
        if np.random.random() < 0.85:
            tx_lat = user_center[0] + np.random.normal(0, 0.1)
            tx_lon = user_center[1] + np.random.normal(0, 0.1)
        else:
            # Giao d·ªãch ·ªü v√πng kh√°c
            other_region = np.random.choice([r for r in GEO_REGIONS.keys() if r != user_region])
            other_center = GEO_REGIONS[other_region]['center']
            tx_lat = other_center[0] + np.random.normal(0, 0.1)
            tx_lon = other_center[1] + np.random.normal(0, 0.1)

        # T√≠nh kho·∫£ng c√°ch (ƒë∆°n gi·∫£n h√≥a: 1 ƒë·ªô ~ 111km)
        location_diff_km = np.sqrt(
            ((tx_lat - user_center[0]) * 111) ** 2 +
            ((tx_lon - user_center[1]) * 111 * np.cos(np.radians(user_center[0]))) ** 2
        )

        transactions.append({
            'transaction_id': f"TX_{i:08d}",
            'user_id': user_id,
            'recipient_id': recipient_id,
            'timestamp': tx_datetime,
            'amount': amount,
            'transaction_type': tx_type,
            'channel': channel,
            'device_id': device_id,
            'is_new_recipient': is_new_recipient,
            'is_new_device': is_new_device,
            'location_diff_km': round(location_diff_km, 2),
            'account_age_days': user['account_age_days'],
            'user_region': user_region,
            'user_preferred_hour': user['preferred_hour'],
            'is_fraud': 0  # M·∫∑c ƒë·ªãnh kh√¥ng ph·∫£i fraud
        })

    df = pd.DataFrame(transactions)
    df = df.sort_values('timestamp').reset_index(drop=True)

    return df, user_profiles, recipient_profiles


# ===========================================================================
# SECTION 2: DERIVED FEATURES (T·ªêI ∆ØU CHO 200K+ D√íNG)
# ===========================================================================

def compute_derived_features_optimized(df, progress_callback=None):
    """
    T√≠nh to√°n c√°c feature ph√°i sinh t·ª´ d·ªØ li·ªáu giao d·ªãch
    PHI√äN B·∫¢N T·ªêI ∆ØU: S·ª≠ d·ª•ng vectorization v√† numpy ƒë·ªÉ x·ª≠ l√Ω nhanh h∆°n
    T·∫•t c·∫£ ƒë·ªÅu l√† past-only (kh√¥ng nh√¨n v√†o t∆∞∆°ng lai)
    """
    df = df.copy()
    df = df.sort_values(['user_id', 'timestamp']).reset_index(drop=True)

    if progress_callback:
        progress_callback("ƒêang t√≠nh amount features...")

    # 1. amount_log: Log c·ªßa s·ªë ti·ªÅn (vectorized)
    df['amount_log'] = np.log1p(df['amount'])

    # 2. amount_tier: Ph√¢n lo·∫°i m·ª©c ti·ªÅn (vectorized v·ªõi np.select)
    conditions = [
        df['amount'] < 100_000,
        df['amount'] < 500_000,
        df['amount'] < 2_000_000,
        df['amount'] < 10_000_000,
    ]
    choices = ['micro', 'small', 'medium', 'large']
    df['amount_tier'] = np.select(conditions, choices, default='very_large')

    # 3. Time features (vectorized)
    df['hour_of_day'] = df['timestamp'].dt.hour + df['timestamp'].dt.minute / 60
    df['day_of_week'] = df['timestamp'].dt.dayofweek
    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)
    df['is_night_hours'] = ((df['hour_of_day'] >= 23) | (df['hour_of_day'] < 6)).astype(int)

    # Vectorized salary/bill period
    days = df['timestamp'].dt.day
    df['is_salary_period'] = ((days >= 25) | (days <= 5)).astype(int)
    df['is_bill_period'] = (days <= 10).astype(int)

    if progress_callback:
        progress_callback("ƒêang t√≠nh amount_vs_avg_user...")

    # 4. amount_vs_avg_user: So s√°nh v·ªõi trung b√¨nh user (past-only, vectorized)
    df['user_cumsum'] = df.groupby('user_id')['amount'].cumsum() - df['amount']
    df['user_cumcount'] = df.groupby('user_id').cumcount()
    df['user_avg_past'] = np.where(
        df['user_cumcount'] > 0,
        df['user_cumsum'] / df['user_cumcount'],
        df['amount']
    )
    df['amount_vs_avg_user'] = np.where(
        df['user_avg_past'] > 0,
        df['amount'] / df['user_avg_past'],
        1.0
    )
    df['amount_vs_avg_user'] = df['amount_vs_avg_user'].clip(0, 100)
    df.drop(['user_cumsum', 'user_cumcount', 'user_avg_past'], axis=1, inplace=True)

    # 5. time_gap_prev_min: Kho·∫£ng c√°ch v·ªõi giao d·ªãch tr∆∞·ªõc (ph√∫t) - vectorized
    df['prev_timestamp'] = df.groupby('user_id')['timestamp'].shift(1)
    df['time_gap_prev_min'] = (df['timestamp'] - df['prev_timestamp']).dt.total_seconds() / 60
    df['time_gap_prev_min'] = df['time_gap_prev_min'].fillna(999999).clip(0, 999999)
    df.drop('prev_timestamp', axis=1, inplace=True)

    if progress_callback:
        progress_callback("ƒêang t√≠nh velocity features (c√≥ th·ªÉ m·∫•t v√†i ph√∫t)...")

    # 6-10. C√°c features c·∫ßn t√≠nh theo user - T·ªêI ∆ØU v·ªõi numba-style approach
    # Kh·ªüi t·∫°o c√°c c·ªôt
    df['velocity_1h'] = 0
    df['velocity_24h'] = 0
    df['recipient_count_30d'] = 0
    df['device_count_30d'] = 0
    df['is_first_large_tx'] = 0
    df['recipient_diversity'] = 0.0

    # Chuy·ªÉn timestamp sang s·ªë ƒë·ªÉ t√≠nh to√°n nhanh h∆°n
    df['ts_numeric'] = df['timestamp'].astype(np.int64) // 10**9  # Unix timestamp

    large_threshold = 5_000_000  # 5 tri·ªáu VND

    # X·ª≠ l√Ω theo batch user ƒë·ªÉ t·ªëi ∆∞u
    user_groups = df.groupby('user_id')
    n_users = len(user_groups)

    for user_idx, (user_id, group) in enumerate(user_groups):
        if progress_callback and user_idx % 500 == 0:
            progress_callback(f"ƒêang x·ª≠ l√Ω user {user_idx}/{n_users}...")

        indices = group.index.values
        ts_values = group['ts_numeric'].values
        recipients = group['recipient_id'].values
        devices = group['device_id'].values
        amounts = group['amount'].values

        # T√≠nh to√°n vectorized trong group
        seen_recipients = set()
        seen_devices_30d = {}
        seen_recipients_30d = {}
        had_large = False

        for i in range(len(indices)):
            idx = indices[i]
            current_ts = ts_values[i]

            # Velocity: ƒë·∫øm giao d·ªãch trong window
            if i > 0:
                time_diffs = (current_ts - ts_values[:i]) / 3600  # Chuy·ªÉn sang gi·ªù
                df.loc[idx, 'velocity_1h'] = np.sum(time_diffs <= 1)
                df.loc[idx, 'velocity_24h'] = np.sum(time_diffs <= 24)

                # recipient_count_30d v√† device_count_30d
                time_diffs_days = time_diffs / 24
                mask_30d = time_diffs_days <= 30
                df.loc[idx, 'recipient_count_30d'] = len(set(recipients[:i][mask_30d]))
                df.loc[idx, 'device_count_30d'] = len(set(devices[:i][mask_30d]))

            # recipient_diversity
            if i > 0:
                df.loc[idx, 'recipient_diversity'] = len(seen_recipients) / i
            seen_recipients.add(recipients[i])

            # is_first_large_tx
            if amounts[i] >= large_threshold and not had_large:
                df.loc[idx, 'is_first_large_tx'] = 1
                had_large = True

    df.drop('ts_numeric', axis=1, inplace=True)

    return df


# ===========================================================================
# SECTION 3: FRAUD & SCAM SCENARIOS (GIAN L·∫¨N + L·ª™A ƒê·∫¢O VI·ªÜT NAM)
# ===========================================================================

def apply_fraud_scenarios(df, fraud_rate=DEFAULT_FRAUD_RATE, seed=RANDOM_SEED):
    """
    √Åp d·ª•ng c√°c k·ªãch b·∫£n GIAN L·∫¨N v√† L·ª™A ƒê·∫¢O theo h√†nh vi Vi·ªát Nam

    GIAN L·∫¨N (Fraud) - K·∫ª gian chi·∫øm ƒëo·∫°t t√†i kho·∫£n:
    1. Account Takeover - B·ªã hack t√†i kho·∫£n
    2. Mule Account - T√†i kho·∫£n trung gian r·ª≠a ti·ªÅn
    3. Card Testing - Test th·∫ª b·ªã ƒë√°nh c·∫Øp

    L·ª™A ƒê·∫¢O (Scam) - N·∫°n nh√¢n t·ª± nguy·ªán chuy·ªÉn ti·ªÅn:
    4. Romance Scam - L·ª´a t√¨nh c·∫£m
    5. Investment Scam - L·ª´a ƒë·∫ßu t∆∞/ti·ªÅn ·∫£o
    6. Impersonation - Gi·∫£ m·∫°o c√¥ng an/ng√¢n h√†ng
    7. Job Scam - L·ª´a vi·ªác l√†m online

    Fraud ch·ªâ ƒë∆∞·ª£c sinh theo scenario - kh√¥ng d·ª±a v√†o ph√¢n b·ªë nh√£n
    """
    np.random.seed(seed)
    df = df.copy()

    n_fraud_target = int(len(df) * fraud_rate)

    # Chia t·ª∑ l·ªá cho c√°c scenario (GIAN L·∫¨N + L·ª™A ƒê·∫¢O)
    scenario_ratios = {
        # === GIAN L·∫¨N (Fraud) ===
        'account_takeover': 0.20,      # B·ªã hack
        'mule_account': 0.15,          # T√†i kho·∫£n trung gian
        'card_testing': 0.10,          # Test th·∫ª
        # === L·ª™A ƒê·∫¢O (Scam) ===
        'romance_scam': 0.15,          # L·ª´a t√¨nh c·∫£m
        'investment_scam': 0.15,       # L·ª´a ƒë·∫ßu t∆∞
        'impersonation_scam': 0.15,    # Gi·∫£ c√¥ng an/ng√¢n h√†ng
        'job_scam': 0.10               # L·ª´a vi·ªác l√†m
    }

    fraud_indices = []

    # =========================================
    # SCENARIO 1: Account Takeover (GIAN L·∫¨N - b·ªã hack)
    # ƒê·∫∑c ƒëi·ªÉm:
    # - ƒê·ªïi thi·∫øt b·ªã ƒë·ªôt ng·ªôt
    # - Giao d·ªãch l√∫c 1-4 AM
    # - Chuy·ªÉn l·ªõn ƒë·∫øn ng∆∞·ªùi l·∫°
    # - V·ªã tr√≠ kh√°c th∆∞·ªùng
    # =========================================
    n_ato = int(n_fraud_target * scenario_ratios['account_takeover'])

    ato_candidates = df[
        (df['is_new_device'] == 1) &
        (df['is_new_recipient'] == 1) &
        (df['amount'] >= 2_000_000)
    ].index.tolist()

    if ato_candidates:
        n_select = min(n_ato, len(ato_candidates))
        selected = np.random.choice(ato_candidates, size=n_select, replace=False)

        for idx in selected:
            df.loc[idx, 'hour_of_day'] = np.random.uniform(1, 4)
            df.loc[idx, 'is_night_hours'] = 1
            df.loc[idx, 'location_diff_km'] = np.random.uniform(100, 500)

            if np.random.random() < np.random.uniform(0.7, 0.9):
                df.loc[idx, 'is_fraud'] = 1
                fraud_indices.append(idx)

    # =========================================
    # SCENARIO 2: Mule Account (GIAN L·∫¨N - t√†i kho·∫£n trung gian)
    # ƒê·∫∑c ƒëi·ªÉm:
    # - Nhi·ªÅu user nh·ªè chuy·ªÉn nhi·ªÅu kho·∫£n nh·ªè
    # - Recipient nh·∫≠n t·ª´ > 20 ng∆∞·ªùi
    # - Velocity cao b·∫•t th∆∞·ªùng
    # =========================================
    n_mule = int(n_fraud_target * scenario_ratios['mule_account'])

    recipient_sender_count = df.groupby('recipient_id')['user_id'].nunique()
    suspicious_recipients = recipient_sender_count[recipient_sender_count > 15].index.tolist()

    mule_candidates = df[
        (df['recipient_id'].isin(suspicious_recipients)) &
        (df['amount'] < 1_000_000) &
        (~df.index.isin(fraud_indices))
    ].index.tolist()

    if mule_candidates:
        n_select = min(n_mule, len(mule_candidates))
        selected = np.random.choice(mule_candidates, size=n_select, replace=False)

        for idx in selected:
            df.loc[idx, 'velocity_1h'] = np.random.randint(5, 15)
            df.loc[idx, 'velocity_24h'] = np.random.randint(20, 50)

            if np.random.random() < np.random.uniform(0.6, 0.9):
                df.loc[idx, 'is_fraud'] = 1
                fraud_indices.append(idx)

    # =========================================
    # SCENARIO 3: Card Testing (GIAN L·∫¨N - test th·∫ª b·ªã ƒë√°nh c·∫Øp)
    # ƒê·∫∑c ƒëi·ªÉm:
    # - Nhi·ªÅu giao d·ªãch nh·ªè (10k-50k)
    # - Nhi·ªÅu recipient trong th·ªùi gian ng·∫Øn
    # - Test xem th·∫ª c√≤n ho·∫°t ƒë·ªông kh√¥ng
    # =========================================
    n_card = int(n_fraud_target * scenario_ratios['card_testing'])

    card_candidates = df[
        (df['amount'] <= 50_000) &
        (df['velocity_1h'] >= 3) &
        (~df.index.isin(fraud_indices))
    ].index.tolist()

    if card_candidates:
        n_select = min(n_card, len(card_candidates))
        selected = np.random.choice(card_candidates, size=n_select, replace=False)

        for idx in selected:
            df.loc[idx, 'recipient_count_30d'] = np.random.randint(10, 30)

            if np.random.random() < np.random.uniform(0.5, 0.8):
                df.loc[idx, 'is_fraud'] = 1
                fraud_indices.append(idx)

    # =========================================
    # SCENARIO 4: Romance Scam (L·ª™A ƒê·∫¢O - l·ª´a t√¨nh c·∫£m)
    # ƒê·∫∑c ƒëi·ªÉm t·∫°i Vi·ªát Nam:
    # - N·∫°n nh√¢n th∆∞·ªùng l√† ph·ª• n·ªØ trung ni√™n, ƒë√†n √¥ng ƒë·ªôc th√¢n
    # - Chuy·ªÉn nhi·ªÅu l·∫ßn, tƒÉng d·∫ßn s·ªë ti·ªÅn
    # - Gi·ªù giao d·ªãch: t·ªëi mu·ªôn (chat v·ªõi "ng∆∞·ªùi y√™u")
    # - L√Ω do: mua qu√†, mua v√© m√°y bay, ƒë·∫ßu t∆∞ chung
    # - S·ªë ti·ªÅn: t·ª´ nh·ªè ƒë·∫øn r·∫•t l·ªõn (1tr - 50tr+)
    # =========================================
    n_romance = int(n_fraud_target * scenario_ratios['romance_scam'])

    # Romance scam: ng∆∞·ªùi nh·∫≠n m·ªõi + s·ªë ti·ªÅn tƒÉng d·∫ßn + gi·ªù t·ªëi
    romance_candidates = df[
        (df['is_new_recipient'] == 1) &
        (df['amount'] >= 1_000_000) &
        (df['amount'] <= 50_000_000) &
        (df['hour_of_day'] >= 19) &  # Gi·ªù t·ªëi (chat v·ªõi "ng∆∞·ªùi y√™u")
        (~df.index.isin(fraud_indices))
    ].index.tolist()

    if romance_candidates:
        n_select = min(n_romance, len(romance_candidates))
        selected = np.random.choice(romance_candidates, size=n_select, replace=False)

        for idx in selected:
            # ƒêi·ªÅu ch·ªânh: th∆∞·ªùng x·∫£y ra bu·ªïi t·ªëi, s·ªë ti·ªÅn tƒÉng d·∫ßn
            df.loc[idx, 'hour_of_day'] = np.random.uniform(20, 23)
            df.loc[idx, 'amount_vs_avg_user'] = np.random.uniform(2, 5)  # Cao h∆°n b√¨nh th∆∞·ªùng

            if np.random.random() < np.random.uniform(0.7, 0.9):
                df.loc[idx, 'is_fraud'] = 1
                fraud_indices.append(idx)

    # =========================================
    # SCENARIO 5: Investment Scam (L·ª™A ƒê·∫¢O - ƒë·∫ßu t∆∞/ti·ªÅn ·∫£o)
    # ƒê·∫∑c ƒëi·ªÉm t·∫°i Vi·ªát Nam:
    # - H·ª©a l·ª£i nhu·∫≠n cao (30-50%/th√°ng)
    # - ƒê·∫ßu t∆∞ forex, crypto, ch·ª©ng kho√°n gi·∫£
    # - N·∫°p ti·ªÅn qua app l·ª´a ƒë·∫£o
    # - S·ªë ti·ªÅn l·ªõn, th∆∞·ªùng l√† ch·∫µn tri·ªáu
    # - Gi·ªù giao d·ªãch: ban ng√†y (sau khi ƒë·ªçc qu·∫£ng c√°o)
    # =========================================
    n_investment = int(n_fraud_target * scenario_ratios['investment_scam'])

    investment_candidates = df[
        (df['is_new_recipient'] == 1) &
        (df['amount'] >= 5_000_000) &  # ƒê·∫ßu t∆∞ th∆∞·ªùng s·ªë ti·ªÅn l·ªõn
        (df['hour_of_day'] >= 8) &
        (df['hour_of_day'] <= 17) &  # Gi·ªù l√†m vi·ªác
        (~df.index.isin(fraud_indices))
    ].index.tolist()

    if investment_candidates:
        n_select = min(n_investment, len(investment_candidates))
        selected = np.random.choice(investment_candidates, size=n_select, replace=False)

        for idx in selected:
            # ƒê·∫ßu t∆∞ scam th∆∞·ªùng l√† s·ªë ch·∫µn tri·ªáu
            df.loc[idx, 'hour_of_day'] = np.random.uniform(9, 16)
            df.loc[idx, 'is_first_large_tx'] = np.random.choice([0, 1], p=[0.4, 0.6])

            if np.random.random() < np.random.uniform(0.75, 0.95):
                df.loc[idx, 'is_fraud'] = 1
                fraud_indices.append(idx)

    # =========================================
    # SCENARIO 6: Impersonation Scam (L·ª™A ƒê·∫¢O - gi·∫£ m·∫°o c√¥ng an/ng√¢n h√†ng)
    # ƒê·∫∑c ƒëi·ªÉm t·∫°i Vi·ªát Nam:
    # - Gi·∫£ c√¥ng an: "d√≠nh l√≠u r·ª≠a ti·ªÅn, chuy·ªÉn ti·ªÅn ƒë·ªÉ ƒëi·ªÅu tra"
    # - Gi·∫£ ng√¢n h√†ng: "t√†i kho·∫£n b·ªã kh√≥a, chuy·ªÉn ƒë·ªÉ x√°c minh"
    # - Gi·∫£ shipper/b∆∞u ƒëi·ªán: "c√≥ ki·ªán h√†ng, thanh to√°n COD"
    # - Th∆∞·ªùng x·∫£y ra ban ng√†y (gi·ªù h√†nh ch√≠nh)
    # - S·ªë ti·ªÅn l·ªõn, chuy·ªÉn g·∫•p trong th·ªùi gian ng·∫Øn
    # - N·∫°n nh√¢n ho·∫£ng lo·∫°n, kh√¥ng suy nghƒ© k·ªπ
    # =========================================
    n_impersonation = int(n_fraud_target * scenario_ratios['impersonation_scam'])

    impersonation_candidates = df[
        (df['is_new_recipient'] == 1) &
        (df['amount'] >= 10_000_000) &  # S·ªë ti·ªÅn l·ªõn
        (df['hour_of_day'] >= 8) &
        (df['hour_of_day'] <= 17) &  # Gi·ªù h√†nh ch√≠nh
        (df['time_gap_prev_min'] < 60) &  # Chuy·ªÉn g·∫•p
        (~df.index.isin(fraud_indices))
    ].index.tolist()

    if impersonation_candidates:
        n_select = min(n_impersonation, len(impersonation_candidates))
        selected = np.random.choice(impersonation_candidates, size=n_select, replace=False)

        for idx in selected:
            # Gi·∫£ c√¥ng an th∆∞·ªùng g·ªçi v√†o gi·ªù h√†nh ch√≠nh
            df.loc[idx, 'hour_of_day'] = np.random.uniform(9, 11.5)  # S√°ng
            df.loc[idx, 'time_gap_prev_min'] = np.random.uniform(5, 30)  # Chuy·ªÉn r·∫•t g·∫•p

            if np.random.random() < np.random.uniform(0.8, 0.95):
                df.loc[idx, 'is_fraud'] = 1
                fraud_indices.append(idx)

    # =========================================
    # SCENARIO 7: Job Scam (L·ª™A ƒê·∫¢O - vi·ªác l√†m online)
    # ƒê·∫∑c ƒëi·ªÉm t·∫°i Vi·ªát Nam:
    # - "L√†m task ki·∫øm ti·ªÅn online"
    # - "N·∫°p ti·ªÅn ƒë·ªÉ nh·∫≠n nhi·ªám v·ª•"
    # - "ƒê·∫∑t c·ªçc ƒë·ªÉ nh·∫≠n vi·ªác"
    # - S·ªë ti·ªÅn nh·ªè ban ƒë·∫ßu, tƒÉng d·∫ßn
    # - Nhi·ªÅu giao d·ªãch trong ng√†y
    # - Target: sinh vi√™n, ng∆∞·ªùi th·∫•t nghi·ªáp
    # =========================================
    n_job = int(n_fraud_target * scenario_ratios['job_scam'])

    job_candidates = df[
        (df['is_new_recipient'] == 1) &
        (df['amount'] >= 100_000) &
        (df['amount'] <= 2_000_000) &  # S·ªë ti·ªÅn v·ª´a ph·∫£i
        (df['velocity_24h'] >= 2) &  # Nhi·ªÅu giao d·ªãch trong ng√†y
        (~df.index.isin(fraud_indices))
    ].index.tolist()

    if job_candidates:
        n_select = min(n_job, len(job_candidates))
        selected = np.random.choice(job_candidates, size=n_select, replace=False)

        for idx in selected:
            df.loc[idx, 'velocity_24h'] = np.random.randint(3, 10)
            df.loc[idx, 'recipient_count_30d'] = np.random.randint(1, 5)

            if np.random.random() < np.random.uniform(0.6, 0.85):
                df.loc[idx, 'is_fraud'] = 1
                fraud_indices.append(idx)

    # =========================================
    # B·ªî SUNG: N·∫øu ch∆∞a ƒë·ªß fraud, th√™m t·ª´ c√°c giao d·ªãch ƒë√°ng ng·ªù
    # =========================================
    remaining = n_fraud_target - len(fraud_indices)
    if remaining > 0:
        suspicious = df[
            (
                (df['is_night_hours'] == 1) |
                (df['is_new_device'] == 1) |
                (df['amount'] >= 10_000_000) |
                (df['velocity_1h'] >= 5) |
                ((df['is_new_recipient'] == 1) & (df['amount'] >= 3_000_000))
            ) &
            (~df.index.isin(fraud_indices))
        ].index.tolist()

        if suspicious:
            n_add = min(remaining, len(suspicious))
            additional = np.random.choice(suspicious, size=n_add, replace=False)
            for idx in additional:
                if np.random.random() < 0.6:
                    df.loc[idx, 'is_fraud'] = 1

    return df


# ===========================================================================
# SECTION 4: RISK FEATURES (KH√îNG LEAK)
# ===========================================================================

def compute_risk_features(df, seed=RANDOM_SEED):
    """
    T√≠nh to√°n c√°c feature risk KH√îNG d·ª±a v√†o label
    """
    np.random.seed(seed)
    df = df.copy()

    # 1. channel_risk: D·ª±a tr√™n rule ng√¢n h√†ng VN + noise
    df['channel_risk'] = df['channel'].map(CHANNEL_RISK_BASE)
    df['channel_risk'] = df['channel_risk'] + np.random.uniform(-0.05, 0.05, len(df))
    df['channel_risk'] = df['channel_risk'].clip(0, 1)

    # 2. tx_type_risk: D·ª±a tr√™n rule VN + noise
    df['tx_type_risk'] = df['transaction_type'].map(TX_TYPE_RISK_BASE)
    df['tx_type_risk'] = df['tx_type_risk'] + np.random.uniform(-0.05, 0.05, len(df))
    df['tx_type_risk'] = df['tx_type_risk'].clip(0, 1)

    # 3. recipient_is_suspicious: Ng∆∞·ªùi nh·∫≠n nh·∫≠n > 20 ng∆∞·ªùi g·ª≠i trong 7 ng√†y
    # T√≠nh s·ªë sender unique cho m·ªói recipient trong 7 ng√†y
    df['tx_date'] = df['timestamp'].dt.date

    recipient_sender_counts = defaultdict(lambda: defaultdict(set))
    df['recipient_is_suspicious'] = 0

    for idx, row in df.iterrows():
        recipient = row['recipient_id']
        sender = row['user_id']
        tx_date = row['timestamp']

        # ƒê·∫øm s·ªë sender trong 7 ng√†y g·∫ßn ƒë√¢y cho recipient n√†y
        cutoff_date = tx_date - timedelta(days=7)
        recent_senders = set()

        for date, senders in recipient_sender_counts[recipient].items():
            if date >= cutoff_date:
                recent_senders.update(senders)

        if len(recent_senders) > 20:
            df.loc[idx, 'recipient_is_suspicious'] = 1

        # C·∫≠p nh·∫≠t tracking
        recipient_sender_counts[recipient][tx_date].add(sender)

    df.drop('tx_date', axis=1, inplace=True)

    # 4. behavioral_risk_score: IsolationForest (unsupervised)
    # Ch·ªâ d√πng c√°c feature kh√¥ng li√™n quan ƒë·∫øn label
    behavior_features = [
        'amount_log', 'hour_of_day', 'velocity_1h', 'velocity_24h',
        'time_gap_prev_min', 'location_diff_km', 'is_night_hours'
    ]

    # Chu·∫©n b·ªã d·ªØ li·ªáu
    X_behavior = df[behavior_features].copy()
    X_behavior['time_gap_prev_min'] = X_behavior['time_gap_prev_min'].clip(0, 10000)
    X_behavior = X_behavior.fillna(0)

    # Chu·∫©n h√≥a
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_behavior)

    # Isolation Forest
    iso_forest = IsolationForest(
        n_estimators=100,
        contamination=0.1,
        random_state=seed,
        n_jobs=-1
    )

    # Score: -1 to 1 -> chuy·ªÉn v·ªÅ 0 to 1
    anomaly_scores = iso_forest.fit_predict(X_scaled)
    decision_scores = iso_forest.decision_function(X_scaled)

    # Chu·∫©n h√≥a v·ªÅ 0-1
    min_score = decision_scores.min()
    max_score = decision_scores.max()
    df['behavioral_risk_score'] = (decision_scores - min_score) / (max_score - min_score + 1e-10)
    df['behavioral_risk_score'] = 1 - df['behavioral_risk_score']  # ƒê·∫£o ng∆∞·ª£c: score cao = risk cao

    # 5. time_context_risk: ƒê·ªô l·ªách so v·ªõi gi·ªù giao d·ªãch quen thu·ªôc
    df['time_context_risk'] = 0.0

    for user_id, group in df.groupby('user_id'):
        indices = group.index.tolist()
        hours = group['hour_of_day'].values
        preferred = group['user_preferred_hour'].values[0] if 'user_preferred_hour' in group.columns else 12

        for i, idx in enumerate(indices):
            # T√≠nh trung b√¨nh gi·ªù c·ªßa c√°c giao d·ªãch tr∆∞·ªõc
            if i > 0:
                past_hours = hours[:i]
                avg_hour = np.mean(past_hours)
                current_hour = hours[i]

                # ƒê·ªô l·ªách (circular difference cho gi·ªù)
                diff = abs(current_hour - avg_hour)
                diff = min(diff, 24 - diff)  # Gi·ªù l√† circular

                # Normalize v·ªÅ 0-1
                df.loc[idx, 'time_context_risk'] = diff / 12.0
            else:
                # Giao d·ªãch ƒë·∫ßu ti√™n: so v·ªõi preferred hour
                diff = abs(hours[i] - preferred)
                diff = min(diff, 24 - diff)
                df.loc[idx, 'time_context_risk'] = diff / 12.0

    df['time_context_risk'] = df['time_context_risk'].clip(0, 1)

    # 6. user_activity_level: Chu·∫©n h√≥a s·ªë giao d·ªãch 30 ng√†y c·ªßa user
    user_tx_counts = df.groupby('user_id').size()
    max_count = user_tx_counts.max()

    df['user_activity_level'] = df['user_id'].map(user_tx_counts) / max_count

    return df


# ===========================================================================
# SECTION 5: FINAL FEATURE ENGINEERING
# ===========================================================================

def prepare_final_dataset(df):
    """
    Chu·∫©n b·ªã dataset cu·ªëi c√πng v·ªõi ƒë·ªß 31 features + label
    """
    # Encode categorical features
    df = df.copy()

    # transaction_type encoding
    tx_type_map = {
        'chuyen_noi_bo': 0,
        'chuyen_lien_ngan_hang': 1,
        'thanh_toan_hoa_don': 2,
        'topup_vi': 3,
        'rut_atm': 4,
        'thanh_toan_pos': 5,
        'hoc_phi_vien_phi': 6,
        'mua_hang_online': 7
    }
    df['transaction_type_encoded'] = df['transaction_type'].map(tx_type_map)

    # channel encoding
    channel_map = {'mobile_app': 0, 'web': 1, 'atm': 2, 'pos': 3}
    df['channel_encoded'] = df['channel'].map(channel_map)

    # amount_tier encoding
    tier_map = {'micro': 0, 'small': 1, 'medium': 2, 'large': 3, 'very_large': 4}
    df['amount_tier_encoded'] = df['amount_tier'].map(tier_map)

    # Danh s√°ch 31 features cu·ªëi c√πng
    final_features = [
        'transaction_type_encoded',  # 1. Lo·∫°i giao d·ªãch (encoded)
        'amount_log',                # 2. Log s·ªë ti·ªÅn
        'amount_tier_encoded',       # 3. M·ª©c ti·ªÅn (encoded)
        'amount_vs_avg_user',        # 4. So v·ªõi trung b√¨nh user
        'channel_encoded',           # 5. K√™nh giao d·ªãch (encoded)
        'channel_risk',              # 6. Risk c·ªßa k√™nh
        'tx_type_risk',              # 7. Risk c·ªßa lo·∫°i giao d·ªãch
        'hour_of_day',               # 8. Gi·ªù trong ng√†y
        'day_of_week',               # 9. Ng√†y trong tu·∫ßn
        'is_weekend',                # 10. C√≥ ph·∫£i cu·ªëi tu·∫ßn
        'is_night_hours',            # 11. Gi·ªù ƒë√™m khuya
        'is_salary_period',          # 12. K·ª≥ l∆∞∆°ng
        'is_bill_period',            # 13. K·ª≥ thanh to√°n h√≥a ƒë∆°n
        'time_gap_prev_min',         # 14. Kho·∫£ng c√°ch giao d·ªãch tr∆∞·ªõc
        'velocity_1h',               # 15. S·ªë giao d·ªãch trong 1h
        'velocity_24h',              # 16. S·ªë giao d·ªãch trong 24h
        'is_new_recipient',          # 17. Ng∆∞·ªùi nh·∫≠n m·ªõi
        'recipient_count_30d',       # 18. S·ªë ng∆∞·ªùi nh·∫≠n 30 ng√†y
        'is_new_device',             # 19. Thi·∫øt b·ªã m·ªõi
        'device_count_30d',          # 20. S·ªë thi·∫øt b·ªã 30 ng√†y
        'location_diff_km',          # 21. Kho·∫£ng c√°ch v·ªã tr√≠
        'account_age_days',          # 22. Tu·ªïi t√†i kho·∫£n
        'is_first_large_tx',         # 23. Giao d·ªãch l·ªõn ƒë·∫ßu ti√™n
        'recipient_is_suspicious',   # 24. Ng∆∞·ªùi nh·∫≠n ƒë√°ng ng·ªù
        'behavioral_risk_score',     # 25. ƒêi·ªÉm risk h√†nh vi
        'time_context_risk',         # 26. Risk ng·ªØ c·∫£nh th·ªùi gian
        'user_activity_level',       # 27. M·ª©c ƒë·ªô ho·∫°t ƒë·ªông user
        'recipient_diversity',       # 28. ƒêa d·∫°ng ng∆∞·ªùi nh·∫≠n
        'amount',                    # 29. S·ªë ti·ªÅn g·ªëc
        'velocity_ratio',            # 30. T·ª∑ l·ªá velocity 1h/24h
        'risk_score_combined'        # 31. ƒêi·ªÉm risk t·ªïng h·ª£p
    ]

    # T√≠nh th√™m c√°c features c√≤n thi·∫øu
    df['velocity_ratio'] = df['velocity_1h'] / (df['velocity_24h'] + 1)

    # risk_score_combined: K·∫øt h·ª£p c√°c risk features (kh√¥ng d√πng label)
    df['risk_score_combined'] = (
        df['channel_risk'] * 0.2 +
        df['tx_type_risk'] * 0.2 +
        df['behavioral_risk_score'] * 0.3 +
        df['time_context_risk'] * 0.15 +
        df['recipient_is_suspicious'] * 0.15
    )

    # Ch·ªçn c√°c c·ªôt c·∫ßn thi·∫øt
    result = df[final_features + ['is_fraud']].copy()

    # Rename ƒë·ªÉ r√µ r√†ng h∆°n
    result = result.rename(columns={
        'transaction_type_encoded': 'transaction_type',
        'channel_encoded': 'channel',
        'amount_tier_encoded': 'amount_tier'
    })

    return result


# ===========================================================================
# SECTION 6: SANITY CHECKS
# ===========================================================================

def run_sanity_checks(df):
    """
    Ki·ªÉm tra ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu
    """
    results = {}

    # 1. Class balance
    fraud_count = df['is_fraud'].sum()
    total_count = len(df)
    fraud_rate = fraud_count / total_count
    results['class_balance'] = {
        'fraud_count': int(fraud_count),
        'non_fraud_count': int(total_count - fraud_count),
        'fraud_rate': round(fraud_rate, 4)
    }

    # 2. Zero variance columns
    zero_var_cols = []
    for col in df.columns:
        if df[col].nunique() <= 1:
            zero_var_cols.append(col)
    results['zero_variance_columns'] = zero_var_cols

    # 3. Correlation v·ªõi is_fraud (ph√°t hi·ªán leak)
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if 'is_fraud' in numeric_cols:
        correlations = {}
        for col in numeric_cols:
            if col != 'is_fraud':
                corr = df[col].corr(df['is_fraud'])
                correlations[col] = round(corr, 4)

        # S·∫Øp x·∫øp theo absolute correlation
        sorted_corr = sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True)
        results['top_correlations'] = sorted_corr[:10]

        # C·∫£nh b√°o n·∫øu correlation qu√° cao (potential leak)
        high_corr = [(k, v) for k, v in sorted_corr if abs(v) > 0.5]
        results['potential_leaks'] = high_corr

    # 4. Missing values
    missing = df.isnull().sum()
    missing_cols = missing[missing > 0].to_dict()
    results['missing_values'] = missing_cols

    # 5. Statistics summary
    results['statistics'] = {
        'total_transactions': len(df),
        'n_features': len(df.columns) - 1,
        'amount_mean': round(df['amount'].mean(), 0) if 'amount' in df.columns else None,
        'amount_median': round(df['amount'].median(), 0) if 'amount' in df.columns else None
    }

    return results


# ===========================================================================
# SECTION 7: QUICK TRAIN LIGHTGBM
# ===========================================================================

def quick_train_lightgbm(df, test_size=0.2, seed=RANDOM_SEED):
    """
    Train nhanh LightGBM v·ªõi time-based split
    """
    try:
        import lightgbm as lgb
        from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score
    except ImportError:
        return None, "LightGBM ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. H√£y ch·∫°y: pip install lightgbm"

    # Chu·∫©n b·ªã d·ªØ li·ªáu
    feature_cols = [col for col in df.columns if col != 'is_fraud']
    X = df[feature_cols].values
    y = df['is_fraud'].values

    # Time-based split (80% train, 20% test - l·∫•y 20% cu·ªëi l√†m test)
    split_idx = int(len(df) * (1 - test_size))
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]

    # T·∫°o dataset
    train_data = lgb.Dataset(X_train, label=y_train, feature_name=feature_cols)
    test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)

    # Parameters
    params = {
        'objective': 'binary',
        'metric': 'auc',
        'boosting_type': 'gbdt',
        'num_leaves': 31,
        'learning_rate': 0.05,
        'feature_fraction': 0.9,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'verbose': -1,
        'seed': seed,
        'is_unbalance': True
    }

    # Train
    model = lgb.train(
        params,
        train_data,
        num_boost_round=200,
        valid_sets=[test_data],
        callbacks=[lgb.early_stopping(stopping_rounds=20)]
    )

    # Predict
    y_pred_proba = model.predict(X_test)
    y_pred = (y_pred_proba >= 0.5).astype(int)

    # Metrics
    metrics = {
        'precision': round(precision_score(y_test, y_pred, zero_division=0), 4),
        'recall': round(recall_score(y_test, y_pred, zero_division=0), 4),
        'f1': round(f1_score(y_test, y_pred, zero_division=0), 4),
        'auc': round(roc_auc_score(y_test, y_pred_proba), 4)
    }

    # Feature importance
    importance = dict(zip(feature_cols, model.feature_importance()))
    importance_sorted = sorted(importance.items(), key=lambda x: x[1], reverse=True)

    return {
        'metrics': metrics,
        'feature_importance': importance_sorted[:15],
        'train_size': len(X_train),
        'test_size': len(X_test),
        'test_fraud_rate': round(y_test.sum() / len(y_test), 4)
    }, None


# ===========================================================================
# SECTION 8: STREAMLIT APP
# ===========================================================================

def main():
    st.set_page_config(
        page_title="üè¶ Synthetic Data Generator - Vietnam Banking",
        page_icon="üè¶",
        layout="wide"
    )

    st.title("üè¶ Synthetic Data Generator cho LightGBM Ch·ªëng Gian L·∫≠n & L·ª´a ƒê·∫£o")
    st.markdown("""
    **·ª®ng d·ª•ng t·∫°o d·ªØ li·ªáu gi·∫£ l·∫≠p chu·∫©n h√†nh vi ng∆∞·ªùi Vi·ªát Nam ƒë·ªÉ train m√¥ h√¨nh ph√°t hi·ªán:**
    - **GIAN L·∫¨N (Fraud)**: Account Takeover, Mule Account, Card Testing
    - **L·ª™A ƒê·∫¢O (Scam)**: Romance Scam, Investment Scam, Gi·∫£ c√¥ng an/ng√¢n h√†ng, Job Scam

    ---
    """)

    # Sidebar - C·∫•u h√¨nh
    st.sidebar.header("‚öôÔ∏è C·∫•u h√¨nh")

    n_transactions = st.sidebar.number_input(
        "S·ªë l∆∞·ª£ng giao d·ªãch",
        min_value=1000,
        max_value=500_000,
        value=DEFAULT_N_TRANSACTIONS,
        step=10000,
        help="S·ªë l∆∞·ª£ng giao d·ªãch c·∫ßn t·∫°o (t·ªëi ƒëa 500.000)"
    )

    n_users = st.sidebar.number_input(
        "S·ªë l∆∞·ª£ng users",
        min_value=100,
        max_value=100_000,
        value=DEFAULT_N_USERS,
        step=500,
        help="S·ªë l∆∞·ª£ng ng∆∞·ªùi d√πng"
    )

    n_recipients = st.sidebar.number_input(
        "S·ªë l∆∞·ª£ng recipients",
        min_value=100,
        max_value=100_000,
        value=DEFAULT_N_RECIPIENTS,
        step=500,
        help="S·ªë l∆∞·ª£ng ng∆∞·ªùi nh·∫≠n"
    )

    fraud_rate = st.sidebar.slider(
        "T·ª∑ l·ªá fraud",
        min_value=0.01,
        max_value=0.20,
        value=DEFAULT_FRAUD_RATE,
        step=0.01,
        help="T·ª∑ l·ªá giao d·ªãch gian l·∫≠n"
    )

    random_seed = st.sidebar.number_input(
        "Random seed",
        min_value=0,
        max_value=99999,
        value=RANDOM_SEED,
        help="Seed ƒë·ªÉ t√°i t·∫°o k·∫øt qu·∫£"
    )

    st.sidebar.markdown("---")

    # Main content
    col1, col2 = st.columns([2, 1])

    with col1:
        st.subheader("üìã Th√¥ng tin Dataset")
        st.markdown(f"""
        - **S·ªë giao d·ªãch:** {n_transactions:,}
        - **S·ªë users:** {n_users:,}
        - **S·ªë recipients:** {n_recipients:,}
        - **T·ª∑ l·ªá fraud m·ª•c ti√™u:** {fraud_rate:.1%}
        - **S·ªë features:** 31 + 1 label
        """)

    with col2:
        st.subheader("üìä 31 Features")
        with st.expander("Xem danh s√°ch features"):
            st.markdown("""
            1. `transaction_type` - Lo·∫°i giao d·ªãch
            2. `amount_log` - Log s·ªë ti·ªÅn
            3. `amount_tier` - M·ª©c ti·ªÅn
            4. `amount_vs_avg_user` - So v·ªõi TB user
            5. `channel` - K√™nh giao d·ªãch
            6. `channel_risk` - Risk k√™nh
            7. `tx_type_risk` - Risk lo·∫°i GD
            8. `hour_of_day` - Gi·ªù trong ng√†y
            9. `day_of_week` - Ng√†y trong tu·∫ßn
            10. `is_weekend` - Cu·ªëi tu·∫ßn
            11. `is_night_hours` - Gi·ªù ƒë√™m
            12. `is_salary_period` - K·ª≥ l∆∞∆°ng
            13. `is_bill_period` - K·ª≥ h√≥a ƒë∆°n
            14. `time_gap_prev_min` - Gap GD tr∆∞·ªõc
            15. `velocity_1h` - Velocity 1h
            16. `velocity_24h` - Velocity 24h
            17. `is_new_recipient` - Recipient m·ªõi
            18. `recipient_count_30d` - S·ªë recipient 30d
            19. `is_new_device` - Device m·ªõi
            20. `device_count_30d` - S·ªë device 30d
            21. `location_diff_km` - Kho·∫£ng c√°ch
            22. `account_age_days` - Tu·ªïi TK
            23. `is_first_large_tx` - GD l·ªõn ƒë·∫ßu ti√™n
            24. `recipient_is_suspicious` - Recipient nghi ng·ªù
            25. `behavioral_risk_score` - Risk h√†nh vi
            26. `time_context_risk` - Risk th·ªùi gian
            27. `user_activity_level` - M·ª©c ho·∫°t ƒë·ªông
            28. `recipient_diversity` - ƒêa d·∫°ng recipient
            29. `amount` - S·ªë ti·ªÅn g·ªëc
            30. `velocity_ratio` - T·ª∑ l·ªá velocity
            31. `risk_score_combined` - Risk t·ªïng h·ª£p
            """)

        st.subheader("üé≠ 7 K·ªãch b·∫£n")
        with st.expander("Xem chi ti·∫øt k·ªãch b·∫£n Fraud/Scam"):
            st.markdown("""
            **GIAN L·∫¨N (Fraud) - K·∫ª gian chi·∫øm TK:**
            1. **Account Takeover** (20%) - B·ªã hack, ƒë·ªïi device, GD l√∫c 1-4AM
            2. **Mule Account** (15%) - TK trung gian r·ª≠a ti·ªÅn
            3. **Card Testing** (10%) - Test th·∫ª b·ªã c·∫Øp

            **L·ª™A ƒê·∫¢O (Scam) - N·∫°n nh√¢n t·ª± chuy·ªÉn:**
            4. **Romance Scam** (15%) - L·ª´a t√¨nh c·∫£m
            5. **Investment Scam** (15%) - L·ª´a ƒë·∫ßu t∆∞/crypto
            6. **Impersonation** (15%) - Gi·∫£ c√¥ng an/NH
            7. **Job Scam** (10%) - L·ª´a vi·ªác l√†m online
            """)

    st.markdown("---")

    # Generate button
    if st.button("üöÄ T·∫°o Dataset", type="primary", use_container_width=True):

        progress_bar = st.progress(0)
        status_text = st.empty()

        try:
            # Step 1: Generate base transactions
            status_text.text("üìù ƒêang t·∫°o giao d·ªãch c∆° b·∫£n...")
            progress_bar.progress(10)

            df, user_profiles, recipient_profiles = generate_base_transactions(
                n_transactions, n_users, n_recipients, random_seed
            )

            # Step 2: Compute derived features (T·ªêI ∆ØU cho 200K+ d√≤ng)
            status_text.text("üî¢ ƒêang t√≠nh to√°n derived features...")
            progress_bar.progress(30)

            def update_status(msg):
                status_text.text(f"üî¢ {msg}")

            df = compute_derived_features_optimized(df, progress_callback=update_status)

            # Step 3: Apply fraud scenarios
            status_text.text("üé≠ ƒêang √°p d·ª•ng fraud scenarios...")
            progress_bar.progress(50)

            df = apply_fraud_scenarios(df, fraud_rate, random_seed)

            # Step 4: Compute risk features
            status_text.text("‚ö†Ô∏è ƒêang t√≠nh to√°n risk features...")
            progress_bar.progress(70)

            df = compute_risk_features(df, random_seed)

            # Step 5: Prepare final dataset
            status_text.text("üì¶ ƒêang chu·∫©n b·ªã dataset cu·ªëi c√πng...")
            progress_bar.progress(85)

            final_df = prepare_final_dataset(df)

            # Store in session state
            st.session_state['generated_data'] = final_df
            st.session_state['raw_data'] = df

            progress_bar.progress(100)
            status_text.text("‚úÖ Ho√†n th√†nh!")

            st.success(f"‚úÖ ƒê√£ t·∫°o th√†nh c√¥ng {len(final_df):,} giao d·ªãch v·ªõi {len(final_df.columns)-1} features!")

        except Exception as e:
            st.error(f"‚ùå L·ªói: {str(e)}")
            import traceback
            st.code(traceback.format_exc())

    # Display results if data exists
    if 'generated_data' in st.session_state:
        final_df = st.session_state['generated_data']

        st.markdown("---")
        st.subheader("üìä K·∫øt qu·∫£")

        # Tabs
        tab1, tab2, tab3, tab4, tab5 = st.tabs([
            "üìã Preview Data",
            "üîç Sanity Checks",
            "üìà Visualizations",
            "ü§ñ Quick Train",
            "üíæ Export"
        ])

        with tab1:
            st.dataframe(final_df.head(100), use_container_width=True)
            st.markdown(f"**Shape:** {final_df.shape}")

            col1, col2 = st.columns(2)
            with col1:
                st.markdown("**Th·ªëng k√™ c∆° b·∫£n:**")
                st.dataframe(final_df.describe().T, use_container_width=True)
            with col2:
                st.markdown("**Data types:**")
                st.dataframe(pd.DataFrame({
                    'Column': final_df.columns,
                    'Type': final_df.dtypes.values,
                    'Non-Null': final_df.count().values
                }), use_container_width=True)

        with tab2:
            st.markdown("### üîç Sanity Checks")

            if st.button("‚ñ∂Ô∏è Ch·∫°y Sanity Checks"):
                checks = run_sanity_checks(final_df)

                # Class balance
                st.markdown("#### 1. Class Balance")
                col1, col2, col3 = st.columns(3)
                col1.metric("Fraud", f"{checks['class_balance']['fraud_count']:,}")
                col2.metric("Non-Fraud", f"{checks['class_balance']['non_fraud_count']:,}")
                col3.metric("Fraud Rate", f"{checks['class_balance']['fraud_rate']:.2%}")

                # Zero variance
                st.markdown("#### 2. Zero Variance Columns")
                if checks['zero_variance_columns']:
                    st.warning(f"‚ö†Ô∏è C√°c c·ªôt c√≥ variance = 0: {checks['zero_variance_columns']}")
                else:
                    st.success("‚úÖ Kh√¥ng c√≥ c·ªôt n√†o c√≥ variance = 0")

                # Correlations
                st.markdown("#### 3. Top Correlations v·ªõi is_fraud")
                if 'top_correlations' in checks:
                    corr_df = pd.DataFrame(checks['top_correlations'], columns=['Feature', 'Correlation'])
                    st.dataframe(corr_df, use_container_width=True)

                # Potential leaks
                st.markdown("#### 4. Potential Data Leaks (|corr| > 0.5)")
                if checks.get('potential_leaks'):
                    st.error(f"‚ö†Ô∏è C·∫£nh b√°o leak ti·ªÅm nƒÉng: {checks['potential_leaks']}")
                else:
                    st.success("‚úÖ Kh√¥ng ph√°t hi·ªán data leak")

                # Missing values
                st.markdown("#### 5. Missing Values")
                if checks['missing_values']:
                    st.warning(f"‚ö†Ô∏è C√°c c·ªôt c√≥ missing: {checks['missing_values']}")
                else:
                    st.success("‚úÖ Kh√¥ng c√≥ missing values")

        with tab3:
            st.markdown("### üìà Visualizations")

            import matplotlib.pyplot as plt

            col1, col2 = st.columns(2)

            with col1:
                # Histogram amount
                st.markdown("#### Ph√¢n ph·ªëi Amount")
                fig1, ax1 = plt.subplots(figsize=(8, 4))

                if 'amount' in final_df.columns:
                    # Log scale cho d·ªÖ nh√¨n
                    ax1.hist(np.log1p(final_df['amount']), bins=50, edgecolor='black', alpha=0.7)
                    ax1.set_xlabel('Log(Amount + 1)')
                    ax1.set_ylabel('Frequency')
                    ax1.set_title('Ph√¢n ph·ªëi Log Amount')
                    st.pyplot(fig1)
                else:
                    st.warning("Kh√¥ng t√¨m th·∫•y c·ªôt amount")

            with col2:
                # Histogram hour_of_day
                st.markdown("#### Ph√¢n ph·ªëi Gi·ªù giao d·ªãch")
                fig2, ax2 = plt.subplots(figsize=(8, 4))

                if 'hour_of_day' in final_df.columns:
                    ax2.hist(final_df['hour_of_day'], bins=24, edgecolor='black', alpha=0.7, color='orange')
                    ax2.set_xlabel('Hour of Day')
                    ax2.set_ylabel('Frequency')
                    ax2.set_title('Ph√¢n ph·ªëi Gi·ªù giao d·ªãch (H√†nh vi VN)')
                    ax2.set_xticks(range(0, 24, 2))
                    st.pyplot(fig2)

            # Fraud by hour
            st.markdown("#### Fraud theo Gi·ªù")
            fig3, ax3 = plt.subplots(figsize=(12, 4))

            hour_fraud = final_df.groupby(final_df['hour_of_day'].astype(int))['is_fraud'].mean()
            ax3.bar(hour_fraud.index, hour_fraud.values, color='red', alpha=0.7)
            ax3.set_xlabel('Hour of Day')
            ax3.set_ylabel('Fraud Rate')
            ax3.set_title('T·ª∑ l·ªá Fraud theo Gi·ªù')
            ax3.set_xticks(range(0, 24))
            st.pyplot(fig3)

            # Fraud by channel
            col1, col2 = st.columns(2)

            with col1:
                st.markdown("#### Fraud theo Channel")
                fig4, ax4 = plt.subplots(figsize=(6, 4))
                channel_fraud = final_df.groupby('channel')['is_fraud'].mean()
                ax4.bar(channel_fraud.index.astype(str), channel_fraud.values, color='purple', alpha=0.7)
                ax4.set_xlabel('Channel')
                ax4.set_ylabel('Fraud Rate')
                ax4.set_title('T·ª∑ l·ªá Fraud theo K√™nh')
                st.pyplot(fig4)

            with col2:
                st.markdown("#### Fraud theo Amount Tier")
                fig5, ax5 = plt.subplots(figsize=(6, 4))
                tier_fraud = final_df.groupby('amount_tier')['is_fraud'].mean()
                ax5.bar(tier_fraud.index.astype(str), tier_fraud.values, color='green', alpha=0.7)
                ax5.set_xlabel('Amount Tier')
                ax5.set_ylabel('Fraud Rate')
                ax5.set_title('T·ª∑ l·ªá Fraud theo M·ª©c ti·ªÅn')
                st.pyplot(fig5)

        with tab4:
            st.markdown("### ü§ñ Quick Train LightGBM")
            st.markdown("""
            Train nhanh m√¥ h√¨nh LightGBM ƒë·ªÉ ki·ªÉm tra ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu.
            S·ª≠ d·ª•ng **time-based split** (80% train / 20% test).
            """)

            if st.button("‚ñ∂Ô∏è Train LightGBM"):
                with st.spinner("ƒêang train..."):
                    result, error = quick_train_lightgbm(final_df, seed=random_seed)

                if error:
                    st.error(error)
                    st.info("C√†i ƒë·∫∑t LightGBM: `pip install lightgbm`")
                else:
                    # Metrics
                    st.markdown("#### üìä Metrics")
                    col1, col2, col3, col4 = st.columns(4)
                    col1.metric("Precision", f"{result['metrics']['precision']:.4f}")
                    col2.metric("Recall", f"{result['metrics']['recall']:.4f}")
                    col3.metric("F1 Score", f"{result['metrics']['f1']:.4f}")
                    col4.metric("AUC", f"{result['metrics']['auc']:.4f}")

                    st.markdown(f"""
                    - **Train size:** {result['train_size']:,}
                    - **Test size:** {result['test_size']:,}
                    - **Test fraud rate:** {result['test_fraud_rate']:.2%}
                    """)

                    # Feature importance
                    st.markdown("#### üèÜ Top 15 Feature Importance")
                    importance_df = pd.DataFrame(
                        result['feature_importance'],
                        columns=['Feature', 'Importance']
                    )

                    fig, ax = plt.subplots(figsize=(10, 6))
                    ax.barh(importance_df['Feature'], importance_df['Importance'], color='steelblue')
                    ax.set_xlabel('Importance')
                    ax.set_title('Feature Importance (LightGBM)')
                    ax.invert_yaxis()
                    st.pyplot(fig)

        with tab5:
            st.markdown("### üíæ Export Dataset")

            # CSV download
            csv = final_df.to_csv(index=False)

            st.download_button(
                label="üì• Download CSV (lightgbm_train_vn.csv)",
                data=csv,
                file_name="lightgbm_train_vn.csv",
                mime="text/csv",
                use_container_width=True
            )

            st.markdown(f"""
            **Th√¥ng tin file:**
            - S·ªë d√≤ng: {len(final_df):,}
            - S·ªë c·ªôt: {len(final_df.columns)} (31 features + 1 label)
            - K√≠ch th∆∞·ªõc ∆∞·ªõc t√≠nh: ~{len(csv) / 1024 / 1024:.1f} MB
            """)

            # Show columns
            st.markdown("**Danh s√°ch c·ªôt:**")
            st.code(", ".join(final_df.columns.tolist()))

    # Footer
    st.markdown("---")
    st.markdown("""
    <div style='text-align: center; color: gray;'>
        üè¶ Synthetic Data Generator for Vietnam Banking Fraud Detection<br>
        Developed with ‚ù§Ô∏è for Vietnamese Banking Industry
    </div>
    """, unsafe_allow_html=True)


if __name__ == "__main__":
    main()
